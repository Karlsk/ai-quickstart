{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c830f6668a34b4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. 什么是 LangChain 的\"链\"？\n",
    "\n",
    "简单理解：链就是把多个处理步骤连接起来，数据从第一步流向最后一步，就像工厂的流水线一样。\n",
    "\n",
    "用生活中的例子来理解\n",
    "想象你要做一杯咖啡：\n",
    "\n",
    "磨豆子 → 2. 冲泡 → 3. 加糖 → 4. 装杯\n",
    "在 LangChain 中，链就是这样的流水线：\n",
    "\n",
    "准备提示词 → 2. 调用LLM → 3. 解析结果 → 4. 返回数据\n",
    "\n",
    "链还能支持 **异步调用**，**流式处理**，**批量处理**，**错误处理和重试** 等高级功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb6a6f116e2cd4d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. LCEL 语法\n",
    "\n",
    "### 2.1 Python 魔术方法基础\n",
    "\n",
    "在 Python 中，当你使用 | 操作符时，实际上调用的是对象的魔术方法：\n",
    "\n",
    "```python\n",
    "# 当你写 a | b 时，Python 实际执行：\n",
    "result = a.__or__(b)\n",
    "\n",
    "# 如果 a 没有 __or__ 方法，Python 会尝试：\n",
    "result = b.__ror__(a)  # ror = reverse or\n",
    "```\n",
    "\n",
    "**LangChain 中的实现**\n",
    "\n",
    "LangChain 的所有 Runnable 对象（包括 prompt、model、parser）都实现了这些魔术方法：\n",
    "\n",
    "```python\n",
    "class BaseRunnable:\n",
    "    def __or__(self, other):\n",
    "        \"\"\"实现 self | other\"\"\"\n",
    "        return RunnableSequence(first=self, last=other)\n",
    "    \n",
    "    def __ror__(self, other):\n",
    "        \"\"\"实现 other | self（当 other 没有 __or__ 时）\"\"\"\n",
    "        return RunnableSequence(first=other, last=self)\n",
    "```\n",
    "\n",
    "**实际效果**\n",
    "\n",
    "\n",
    "```python\n",
    "# 这三种写法是等价的：\n",
    "chain1 = prompt | model | parser\n",
    "chain2 = prompt.__or__(model).__or__(parser)\n",
    "chain3 = RunnableSequence(first=prompt, last=RunnableSequence(first=model, last=parser))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3280a530fa283e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# prompt.__or__(model) 的伪代码实现\n",
    "\n",
    "class PromptTemplate(BaseRunnable):\n",
    "    def __init__(self, template, input_variables):\n",
    "        self.template = template\n",
    "        self.input_variables = input_variables\n",
    "    \n",
    "    def __or__(self, other):\n",
    "        \"\"\"\n",
    "        实现 prompt | model 的核心逻辑\n",
    "        \"\"\"\n",
    "        \n",
    "        return RunnableSequence(first=self, last=other)\n",
    "    \n",
    "    def invoke(self, inputs):\n",
    "        # 简单的字符串格式化\n",
    "        return self.template.format(**inputs)\n",
    "\n",
    "class RunnableSequence(BaseRunnable):\n",
    "    def __init__(self, first, last):\n",
    "        self.first = first\n",
    "        self.last = last\n",
    "        \n",
    "    def invoke(self, inputs):\n",
    "        \"\"\"\n",
    "        执行序列链的核心逻辑\n",
    "        \"\"\"\n",
    "        \n",
    "        # 步骤1：执行第一个组件\n",
    "        intermediate_result = self.first.invoke(inputs)\n",
    "        \n",
    "        # 步骤2：将第一个组件的输出作为第二个组件的输入\n",
    "        final_result = self.last.invoke(intermediate_result)\n",
    "        \n",
    "        return final_result\n",
    "    \n",
    "    def __or__(self, other):\n",
    "        \"\"\"\n",
    "        实现 (prompt | model) | parser 的逻辑\n",
    "        \"\"\"\n",
    "        \n",
    "        return RunnableSequence(first=self, last=other)\n",
    "    \n",
    "\n",
    "# 使用示例的内部执行流程\n",
    "prompt = PromptTemplate(\"分析：{text}\", [\"text\"])\n",
    "model = SomeLLM()\n",
    "\n",
    "# 当执行 prompt | model 时：\n",
    "chain = prompt.__or__(model)  # 返回 RunnableSequence(first=prompt, last=model)\n",
    "\n",
    "# 当执行 chain.invoke({\"text\": \"项目进度\"}) 时：\n",
    "# 1. 调用 RunnableSequence.invoke({\"text\": \"项目进度\"})\n",
    "# 2. intermediate_result = prompt.invoke({\"text\": \"项目进度\"})  # 返回 \"分析：项目进度\"\n",
    "# 3. final_result = model.invoke(\"分析：项目进度\")  # 返回 LLM 的响应\n",
    "# 4. 返回 final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345541139845df0a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 完整的三步链：prompt | model | parser\n",
    "def create_three_step_chain():\n",
    "    prompt = PromptTemplate(\"分析：{text}\", [\"text\"])\n",
    "    model = SomeLLM()\n",
    "    parser = SomeParser()\n",
    "    \n",
    "    # 第一步：prompt | model\n",
    "    step1_chain = prompt.__or__(model)\n",
    "    \n",
    "    # 第二步：(prompt | model) | parser\n",
    "    final_chain = step1_chain.__or__(parser)\n",
    "    \n",
    "    # 等价于：RunnableSequence(first=step1_chain, last=parser)\n",
    "    \n",
    "    return final_chain\n",
    "\n",
    "# 执行时的内部流程\n",
    "def execute_chain(inputs):\n",
    "    # inputs = {\"text\": \"项目进度\"}\n",
    "    \n",
    "    # 第一层 RunnableSequence.invoke()\n",
    "    # first = RunnableSequence(prompt, model)\n",
    "    # last = parser\n",
    "    \n",
    "    # 执行 first.invoke(inputs)\n",
    "    # 这会触发第二层 RunnableSequence.invoke()\n",
    "    # first = prompt, last = model\n",
    "    \n",
    "    # 执行 prompt.invoke({\"text\": \"项目进度\"})\n",
    "    prompt_result = \"分析：项目进度\"\n",
    "    \n",
    "    # 执行 model.invoke(\"分析：项目进度\")\n",
    "    model_result = \"这是一个关于项目进度的分析...\"\n",
    "    \n",
    "    # 第一层继续执行 last.invoke(model_result)\n",
    "    # 执行 parser.invoke(\"这是一个关于项目进度的分析...\")\n",
    "    final_result = {\"analysis\": \"项目进度良好\"}\n",
    "    \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f445ee1a74e68e40",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3 其他问题\n",
    "\n",
    "为什么需要 __ror__？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04448aa69de733b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 假设你有一个自定义函数想要加入链中\n",
    "def custom_preprocessor(text):\n",
    "    return text.upper()\n",
    "\n",
    "# 如果 custom_preprocessor 没有 __or__ 方法\n",
    "# 但 prompt 有 __ror__ 方法，这样就能工作：\n",
    "chain = custom_preprocessor | prompt | model\n",
    "\n",
    "# Python 会调用：\n",
    "# prompt.__ror__(custom_preprocessor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42dd96cc9aa10ec",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Runnable 对象接口\n",
    "\n",
    "Runnable 对象是一个可以被调用、异步执行、批处理、流式处理、并行处理的工作单元，通过schema属性、run方法定义。\n",
    "\n",
    "\n",
    "1. Runnable 核心执行方法:\n",
    "\n",
    "基础执行方法\n",
    "\n",
    "```python\n",
    "# 同步执行\n",
    "result = runnable.invoke(input)\n",
    "\n",
    "# 异步执行\n",
    "result = await runnable.ainvoke(input)\n",
    "\n",
    "# 批量执行\n",
    "results = runnable.batch([input1, input2, input3])\n",
    "results = await runnable.abatch([input1, input2, input3])\n",
    "\n",
    "# 流式执行\n",
    "for chunk in runnable.stream(input):\n",
    "    print(chunk)\n",
    "\n",
    "async for chunk in runnable.astream(input):\n",
    "    print(chunk)\n",
    "```\n",
    "\n",
    "输入输出类型\n",
    "\n",
    "```python\n",
    "# Runnable 是泛型，定义输入输出类型\n",
    "class MyRunnable(Runnable[Dict, str]):  # 输入Dict，输出str\n",
    "    def invoke(self, input: Dict) -> str:\n",
    "        return f\"处理结果: {input}\"\n",
    "```\n",
    "\n",
    "2. 链式组合操作\n",
    "管道操作符 |\n",
    "\n",
    "```python\n",
    "# 串行组合\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# 等价于\n",
    "chain = RunnableSequence(first=prompt, last=RunnableSequence(first=model, last=parser))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976043ad854e3fab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1 schema 属性的作用\n",
    "\n",
    "#### 每个 Runnable 都有这两个属性\n",
    "runnable.input_schema   # 定义期望的输入格式\n",
    "\n",
    "runnable.output_schema  # 定义输出的数据格式\n",
    "\n",
    "#### 主要用途\n",
    "- 类型检查: 验证输入数据是否符合预期格式\n",
    "- 文档生成: 自动生成API文档\n",
    "- IDE支持: 提供代码补全和类型提示\n",
    "- 调试帮助: 快速定位数据格式问题\n",
    "\n",
    "### 3.2 Schema 的数据结构\n",
    "Schema 通常是 JSON Schema 格式或 Pydantic 模型：\n",
    "\n",
    "```python\n",
    "# JSON Schema 格式示例\n",
    "{\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"text\": {\"type\": \"string\"},\n",
    "        \"temperature\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 2}\n",
    "    },\n",
    "    \"required\": [\"text\"]\n",
    "}\n",
    "\n",
    "# Pydantic 模型格式\n",
    "class InputModel(BaseModel):\n",
    "    text: str\n",
    "    temperature: float = 0.7\n",
    "```\n",
    "### 3.3 实际例子 PromptTemplate 的 Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da04349e8ea0e59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T06:48:09.769559Z",
     "start_time": "2025-12-15T06:48:09.506783Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入 Schema:\n",
      "{'properties': {'age': {'title': 'Age', 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['age', 'name'], 'title': 'PromptInput', 'type': 'object'}\n",
      "输出 Schema:\n",
      "{'$defs': {'AIMessage': {'additionalProperties': True, 'description': 'Message from an AI.\\n\\nAn `AIMessage` is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model and standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'additionalProperties': True, 'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'additionalProperties': True, 'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'additionalProperties': True, 'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'ai', 'default': 'ai', 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}, 'tool_calls': {'default': [], 'items': {'$ref': '#/$defs/ToolCall'}, 'title': 'Tool Calls', 'type': 'array'}, 'invalid_tool_calls': {'default': [], 'items': {'$ref': '#/$defs/InvalidToolCall'}, 'title': 'Invalid Tool Calls', 'type': 'array'}, 'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'}, {'type': 'null'}], 'default': None}}, 'required': ['content'], 'title': 'AIMessage', 'type': 'object'}, 'AIMessageChunk': {'additionalProperties': True, 'description': 'Message chunk from an AI (yielded when streaming).', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'additionalProperties': True, 'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'additionalProperties': True, 'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'additionalProperties': True, 'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'AIMessageChunk', 'default': 'AIMessageChunk', 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}, 'tool_calls': {'default': [], 'items': {'$ref': '#/$defs/ToolCall'}, 'title': 'Tool Calls', 'type': 'array'}, 'invalid_tool_calls': {'default': [], 'items': {'$ref': '#/$defs/InvalidToolCall'}, 'title': 'Invalid Tool Calls', 'type': 'array'}, 'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'}, {'type': 'null'}], 'default': None}, 'tool_call_chunks': {'default': [], 'items': {'$ref': '#/$defs/ToolCallChunk'}, 'title': 'Tool Call Chunks', 'type': 'array'}, 'chunk_position': {'anyOf': [{'const': 'last', 'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Chunk Position'}}, 'required': ['content'], 'title': 'AIMessageChunk', 'type': 'object'}, 'ChatMessage': {'additionalProperties': True, 'description': 'Message that can be assigned an arbitrary speaker (i.e. role).', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'additionalProperties': True, 'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'additionalProperties': True, 'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'additionalProperties': True, 'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'chat', 'default': 'chat', 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}, 'role': {'title': 'Role', 'type': 'string'}}, 'required': ['content', 'role'], 'title': 'ChatMessage', 'type': 'object'}, 'ChatMessageChunk': {'additionalProperties': True, 'description': 'Chat Message chunk.', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'additionalProperties': True, 'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'additionalProperties': True, 'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'additionalProperties': True, 'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'ChatMessageChunk', 'default': 'ChatMessageChunk', 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}, 'role': {'title': 'Role', 'type': 'string'}}, 'required': ['content', 'role'], 'title': 'ChatMessageChunk', 'type': 'object'}, 'ChatPromptValueConcrete': {'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\n\\nFor use in external schemas.', 'properties': {'messages': {'items': {'oneOf': [{'$ref': '#/$defs/AIMessage'}, {'$ref': '#/$defs/HumanMessage'}, {'$ref': '#/$defs/ChatMessage'}, {'$ref': '#/$defs/SystemMessage'}, {'$ref': '#/$defs/FunctionMessage'}, {'$ref': '#/$defs/ToolMessage'}, {'$ref': '#/$defs/AIMessageChunk'}, {'$ref': '#/$defs/HumanMessageChunk'}, {'$ref': '#/$defs/ChatMessageChunk'}, {'$ref': '#/$defs/SystemMessageChunk'}, {'$ref': '#/$defs/FunctionMessageChunk'}, {'$ref': '#/$defs/ToolMessageChunk'}]}, 'title': 'Messages', 'type': 'array'}, 'type': {'const': 'ChatPromptValueConcrete', 'default': 'ChatPromptValueConcrete', 'title': 'Type', 'type': 'string'}}, 'required': ['messages'], 'title': 'ChatPromptValueConcrete', 'type': 'object'}, 'FunctionMessage': {'additionalProperties': True, 'description': 'Message for passing the result of executing a tool back to a model.\\n\\n`FunctionMessage` are an older version of the `ToolMessage` schema, and\\ndo not contain the `tool_call_id` field.\\n\\nThe `tool_call_id` field is used to associate the tool call request with the\\ntool call response. Useful in situations where a chat model is able\\nto request multiple tool calls in parallel.', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'additionalProperties': True, 'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'additionalProperties': True, 'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'additionalProperties': True, 'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'function', 'default': 'function', 'title': 'Type', 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}}, 'required': ['content', 'name'], 'title': 'FunctionMessage', 'type': 'object'}, 'FunctionMessageChunk': {'additionalProperties': True, 'description': 'Function Message chunk.', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'additionalProperties': True, 'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'additionalProperties': True, 'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'additionalProperties': True, 'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'FunctionMessageChunk', 'default': 'FunctionMessageChunk', 'title': 'Type', 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}}, 'required': ['content', 'name'], 'title': 'FunctionMessageChunk', 'type': 'object'}, 'HumanMessage': {'additionalProperties': True, 'description': 'Message from the user.\\n\\nA `HumanMessage` is a message that is passed in from a user to the model.\\n\\nExample:\\n    ```python\\n    from langchain_core.messages import HumanMessage, SystemMessage\\n\\n    messages = [\\n        SystemMessage(content=\"You are a helpful assistant! Your name is Bob.\"),\\n        HumanMessage(content=\"What is your name?\"),\\n    ]\\n\\n    # Instantiate a chat model and invoke it with the messages\\n    model = ...\\n    print(model.invoke(messages))\\n    ```', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'additionalProperties': True, 'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'additionalProperties': True, 'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'additionalProperties': True, 'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'human', 'default': 'human', 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}}, 'required': ['content'], 'title': 'HumanMessage', 'type': 'object'}, 'HumanMessageChunk': {'additionalProperties': True, 'description': 'Human Message chunk.', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'additionalProperties': True, 'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'additionalProperties': True, 'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'additionalProperties': True, 'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'HumanMessageChunk', 'default': 'HumanMessageChunk', 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}}, 'required': ['content'], 'title': 'HumanMessageChunk', 'type': 'object'}, 'InputTokenDetails': {'description': 'Breakdown of input token counts.\\n\\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\\n\\nExample:\\n    ```python\\n    {\\n        \"audio\": 10,\\n        \"cache_creation\": 200,\\n        \"cache_read\": 100,\\n    }\\n    ```\\n\\nMay also hold extra provider-specific keys.\\n\\n!!! version-added \"Added in `langchain-core` 0.3.9\"', 'properties': {'audio': {'title': 'Audio', 'type': 'integer'}, 'cache_creation': {'title': 'Cache Creation', 'type': 'integer'}, 'cache_read': {'title': 'Cache Read', 'type': 'integer'}}, 'title': 'InputTokenDetails', 'type': 'object'}, 'InvalidToolCall': {'description': 'Allowance for errors made by LLM.\\n\\nHere we add an `error` key to surface errors made during generation\\n(e.g., invalid JSON arguments.)', 'properties': {'type': {'const': 'invalid_tool_call', 'title': 'Type', 'type': 'string'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Name'}, 'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'}, 'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Error'}, 'index': {'anyOf': [{'type': 'integer'}, {'type': 'string'}], 'title': 'Index'}, 'extras': {'additionalProperties': True, 'title': 'Extras', 'type': 'object'}}, 'required': ['type', 'id', 'name', 'args', 'error'], 'title': 'InvalidToolCall', 'type': 'object'}, 'OutputTokenDetails': {'description': 'Breakdown of output token counts.\\n\\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\\n\\nExample:\\n    ```python\\n    {\\n        \"audio\": 10,\\n        \"reasoning\": 200,\\n    }\\n    ```\\n\\nMay also hold extra provider-specific keys.\\n\\n!!! version-added \"Added in `langchain-core` 0.3.9\"', 'properties': {'audio': {'title': 'Audio', 'type': 'integer'}, 'reasoning': {'title': 'Reasoning', 'type': 'integer'}}, 'title': 'OutputTokenDetails', 'type': 'object'}, 'StringPromptValue': {'description': 'String prompt value.', 'properties': {'text': {'title': 'Text', 'type': 'string'}, 'type': {'const': 'StringPromptValue', 'default': 'StringPromptValue', 'title': 'Type', 'type': 'string'}}, 'required': ['text'], 'title': 'StringPromptValue', 'type': 'object'}, 'SystemMessage': {'additionalProperties': True, 'description': 'Message for priming AI behavior.\\n\\nThe system message is usually passed in as the first of a sequence\\nof input messages.\\n\\nExample:\\n    ```python\\n    from langchain_core.messages import HumanMessage, SystemMessage\\n\\n    messages = [\\n        SystemMessage(content=\"You are a helpful assistant! Your name is Bob.\"),\\n        HumanMessage(content=\"What is your name?\"),\\n    ]\\n\\n    # Define a chat model and invoke it with the messages\\n    print(model.invoke(messages))\\n    ```', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'additionalProperties': True, 'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'additionalProperties': True, 'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'additionalProperties': True, 'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'system', 'default': 'system', 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}}, 'required': ['content'], 'title': 'SystemMessage', 'type': 'object'}, 'SystemMessageChunk': {'additionalProperties': True, 'description': 'System Message chunk.', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'additionalProperties': True, 'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'additionalProperties': True, 'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'additionalProperties': True, 'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'SystemMessageChunk', 'default': 'SystemMessageChunk', 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}}, 'required': ['content'], 'title': 'SystemMessageChunk', 'type': 'object'}, 'ToolCall': {'description': 'Represents an AI\\'s request to call a tool.\\n\\nExample:\\n    ```python\\n    {\"name\": \"foo\", \"args\": {\"a\": 1}, \"id\": \"123\"}\\n    ```\\n\\n    This represents a request to call the tool named `\\'foo\\'` with arguments\\n    `{\"a\": 1}` and an identifier of `\\'123\\'`.', 'properties': {'name': {'title': 'Name', 'type': 'string'}, 'args': {'additionalProperties': True, 'title': 'Args', 'type': 'object'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'}, 'type': {'const': 'tool_call', 'title': 'Type', 'type': 'string'}}, 'required': ['name', 'args', 'id'], 'title': 'ToolCall', 'type': 'object'}, 'ToolCallChunk': {'description': 'A chunk of a tool call (yielded when streaming).\\n\\nWhen merging `ToolCallChunk`s (e.g., via `AIMessageChunk.__add__`),\\nall string attributes are concatenated. Chunks are only merged if their\\nvalues of `index` are equal and not None.\\n\\nExample:\\n```python\\nleft_chunks = [ToolCallChunk(name=\"foo\", args=\\'{\"a\":\\', index=0)]\\nright_chunks = [ToolCallChunk(name=None, args=\"1}\", index=0)]\\n\\n(\\n    AIMessageChunk(content=\"\", tool_call_chunks=left_chunks)\\n    + AIMessageChunk(content=\"\", tool_call_chunks=right_chunks)\\n).tool_call_chunks == [ToolCallChunk(name=\"foo\", args=\\'{\"a\":1}\\', index=0)]\\n```', 'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Name'}, 'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'}, 'index': {'anyOf': [{'type': 'integer'}, {'type': 'null'}], 'title': 'Index'}, 'type': {'const': 'tool_call_chunk', 'title': 'Type', 'type': 'string'}}, 'required': ['name', 'args', 'id', 'index'], 'title': 'ToolCallChunk', 'type': 'object'}, 'ToolMessage': {'additionalProperties': True, 'description': 'Message for passing the result of executing a tool back to a model.\\n\\n`ToolMessage` objects contain the result of a tool invocation. Typically, the result\\nis encoded inside the `content` field.\\n\\nExample: A `ToolMessage` representing a result of `42` from a tool call with id\\n\\n```python\\nfrom langchain_core.messages import ToolMessage\\n\\nToolMessage(content=\"42\", tool_call_id=\"call_Jja7J89XsjrOLA5r!MEOW!SL\")\\n```\\n\\nExample: A `ToolMessage` where only part of the tool output is sent to the model\\nand the full output is passed in to artifact.\\n\\n```python\\nfrom langchain_core.messages import ToolMessage\\n\\ntool_output = {\\n    \"stdout\": \"From the graph we can see that the correlation between \"\\n    \"x and y is ...\",\\n    \"stderr\": None,\\n    \"artifacts\": {\"type\": \"image\", \"base64_data\": \"/9j/4gIcSU...\"},\\n}\\n\\nToolMessage(\\n    content=tool_output[\"stdout\"],\\n    artifact=tool_output,\\n    tool_call_id=\"call_Jja7J89XsjrOLA5r!MEOW!SL\",\\n)\\n```\\n\\nThe `tool_call_id` field is used to associate the tool call request with the\\ntool call response. Useful in situations where a chat model is able\\nto request multiple tool calls in parallel.', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'additionalProperties': True, 'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'additionalProperties': True, 'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'additionalProperties': True, 'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'tool', 'default': 'tool', 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}, 'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}, 'artifact': {'default': None, 'title': 'Artifact'}, 'status': {'default': 'success', 'enum': ['success', 'error'], 'title': 'Status', 'type': 'string'}}, 'required': ['content', 'tool_call_id'], 'title': 'ToolMessage', 'type': 'object'}, 'ToolMessageChunk': {'additionalProperties': True, 'description': 'Tool Message chunk.', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'additionalProperties': True, 'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'additionalProperties': True, 'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'additionalProperties': True, 'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'ToolMessageChunk', 'default': 'ToolMessageChunk', 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}, 'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}, 'artifact': {'default': None, 'title': 'Artifact'}, 'status': {'default': 'success', 'enum': ['success', 'error'], 'title': 'Status', 'type': 'string'}}, 'required': ['content', 'tool_call_id'], 'title': 'ToolMessageChunk', 'type': 'object'}, 'UsageMetadata': {'description': 'Usage metadata for a message, such as token counts.\\n\\nThis is a standard representation of token usage that is consistent across models.\\n\\nExample:\\n    ```python\\n    {\\n        \"input_tokens\": 350,\\n        \"output_tokens\": 240,\\n        \"total_tokens\": 590,\\n        \"input_token_details\": {\\n            \"audio\": 10,\\n            \"cache_creation\": 200,\\n            \"cache_read\": 100,\\n        },\\n        \"output_token_details\": {\\n            \"audio\": 10,\\n            \"reasoning\": 200,\\n        },\\n    }\\n    ```\\n\\n!!! warning \"Behavior changed in `langchain-core` 0.3.9\"\\n    Added `input_token_details` and `output_token_details`.\\n\\n!!! note \"LangSmith SDK\"\\n    The LangSmith SDK also has a `UsageMetadata` class. While the two share fields,\\n    LangSmith\\'s `UsageMetadata` has additional fields to capture cost information\\n    used by the LangSmith platform.', 'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'}, 'output_tokens': {'title': 'Output Tokens', 'type': 'integer'}, 'total_tokens': {'title': 'Total Tokens', 'type': 'integer'}, 'input_token_details': {'$ref': '#/$defs/InputTokenDetails'}, 'output_token_details': {'$ref': '#/$defs/OutputTokenDetails'}}, 'required': ['input_tokens', 'output_tokens', 'total_tokens'], 'title': 'UsageMetadata', 'type': 'object'}}, 'anyOf': [{'$ref': '#/$defs/StringPromptValue'}, {'$ref': '#/$defs/ChatPromptValueConcrete'}], 'title': 'PromptTemplateOutput'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hn/5h0gmdv550d1_jd5nssyp2p40000gn/T/ipykernel_12398/841185670.py:10: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  print(prompt.input_schema.schema())\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"name\", \"age\"],\n",
    "    template=\"你好，我是{name}，今年{age}岁\"\n",
    ")\n",
    "\n",
    "# 查看输入 schema\n",
    "print(\"输入 Schema:\")\n",
    "print(prompt.input_schema.schema())\n",
    "\n",
    "# 查看输出 schema\n",
    "print(\"输出 Schema:\")\n",
    "print(prompt.output_schema.schema())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa976e06cc873c3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**练习**\n",
    "1. 掌握 LLM 的 Schema\n",
    "2. 掌握链式组合的 Schema\n",
    "3. 并行组合的 Schema （后面讲并行）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5940066b9e2421",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 常见的应用场景\n",
    "\n",
    "```python\n",
    "# API 文档生成\n",
    "def generate_api_docs(runnable):\n",
    "    \"\"\"根据 schema 生成 API 文档\"\"\"\n",
    "    input_schema = runnable.input_schema.schema()\n",
    "    output_schema = runnable.output_schema.schema()\n",
    "    \n",
    "    docs = f\"\"\"\n",
    "    API 接口文档:\n",
    "    \n",
    "    输入格式:\n",
    "    {json.dumps(input_schema, indent=2, ensure_ascii=False)}\n",
    "    \n",
    "    输出格式:\n",
    "    {json.dumps(output_schema, indent=2, ensure_ascii=False)}\n",
    "    \"\"\"\n",
    "    return docs\n",
    "\n",
    "# 输入验证\n",
    "def validate_input(runnable, input_data):\n",
    "    \"\"\"验证输入数据是否符合 schema\"\"\"\n",
    "    try:\n",
    "        # 使用 schema 验证输入\n",
    "        validated = runnable.input_schema(**input_data)\n",
    "        return True, validated\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "# 使用示例\n",
    "is_valid, result = validate_input(analyzer, {\n",
    "    \"name\": \"张三\",\n",
    "    \"age\": 30,\n",
    "    \"email\": \"zhangsan@example.com\"\n",
    "})\n",
    "\n",
    "# 类型安全的调用\n",
    "def safe_invoke(runnable, input_data):\n",
    "    \"\"\"类型安全的调用方法\"\"\"\n",
    "    # 验证输入\n",
    "    is_valid, validated_input = validate_input(runnable, input_data)\n",
    "    if not is_valid:\n",
    "        raise ValueError(f\"输入验证失败: {validated_input}\")\n",
    "    \n",
    "    # 执行调用\n",
    "    result = runnable.invoke(validated_input)\n",
    "    \n",
    "    # 可以进一步验证输出格式\n",
    "    return result\n",
    "\n",
    "# 调试\n",
    "# 快速查看 schema\n",
    "def inspect_runnable(runnable):\n",
    "    print(f\"组件类型: {type(runnable).__name__}\")\n",
    "    print(f\"输入类型: {runnable.input_schema}\")\n",
    "    print(f\"输出类型: {runnable.output_schema}\")\n",
    "    \n",
    "    # 如果是链，递归查看每个组件\n",
    "    if hasattr(runnable, 'steps'):\n",
    "        for i, step in enumerate(runnable.steps):\n",
    "            print(f\"步骤 {i+1}: {type(step).__name__}\")\n",
    "\n",
    "# 测试数据生成\n",
    "def generate_test_data(schema):\n",
    "    \"\"\"根据 schema 生成测试数据\"\"\"\n",
    "    # 这里可以实现根据 schema 自动生成测试数据的逻辑\n",
    "    pass\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4195d9993f428fc4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### ainvoke 异步方法\n",
    "\n",
    "#### 1. ainvoke() 方法的基本原理\n",
    "为什么需要 ainvoke()？\n",
    "\n",
    "```python\n",
    "# 同步调用 - 会阻塞当前线程\n",
    "result = runnable.invoke(input)  # 等待几秒钟才返回\n",
    "\n",
    "# 异步调用 - 不阻塞，可以并发执行其他任务\n",
    "result = await runnable.ainvoke(input)  # 非阻塞执行\n",
    "```\n",
    "\n",
    "ainvoke() 的声明结构\n",
    "```python\n",
    "class BaseRunnable:\n",
    "    async def ainvoke(self, input, config=None):\n",
    "        \"\"\"异步版本的 invoke 方法\"\"\"\n",
    "        # 如果子类没有实现异步版本，就用线程池执行同步版本\n",
    "        loop = asyncio.get_running_loop()\n",
    "        return await loop.run_in_executor(\n",
    "            None,  # 使用默认线程池\n",
    "            self.invoke,  # 要执行的同步函数\n",
    "            input,  # 传递给 invoke 的参数\n",
    "            config\n",
    "        )\n",
    "```\n",
    "\n",
    "#### 2. 执行流程详解\n",
    "\n",
    "步骤1：获取事件循环  \n",
    "\n",
    "loop = asyncio.get_running_loop()\n",
    "\n",
    "作用：\n",
    "\n",
    "- 获取当前正在运行的异步事件循环\n",
    "- 事件循环是异步程序的核心，负责调度和执行异步任务\n",
    "- 如果没有运行中的事件循环，会抛出 RuntimeError\n",
    "\n",
    "步骤2：使用线程池执行器\n",
    "\n",
    "await loop.run_in_executor(None, self.invoke, input, config)\n",
    "\n",
    "参数解释：\n",
    "\n",
    "- None：使用默认的 ThreadPoolExecutor\n",
    "- self.invoke：要在线程池中执行的同步函数\n",
    "- input, config：传递给函数的参数\n",
    "\n",
    "步骤3：线程池执行\n",
    "\n",
    "```python\n",
    "# 内部执行过程（简化版）\n",
    "def run_in_executor(executor, func, *args):\n",
    "    # 1. 将同步函数提交到线程池\n",
    "    future = executor.submit(func, *args)\n",
    "    \n",
    "    # 2. 将线程池的 Future 转换为 asyncio.Future\n",
    "    asyncio_future = asyncio.wrap_future(future)\n",
    "    \n",
    "    # 3. 返回可等待的 Future 对象\n",
    "    return asyncio_future\n",
    "```\n",
    "\n",
    "#### 3. 完整的执行示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c88f374a5e288e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-15T07:10:08.653866Z",
     "start_time": "2025-12-15T07:10:06.641204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "异步调用开始: 任务1\n",
      "获取到事件循环: <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "提交到线程池执行...\n",
      "异步调用开始: 任务2\n",
      "获取到事件循环: <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "提交到线程池执行...\n",
      "异步调用开始: 任务3\n",
      "获取到事件循环: <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "提交到线程池执行...\n",
      "开始处理: 任务1\n",
      "开始处理: 任务2\n",
      "开始处理: 任务3\n",
      "处理完成: 任务2处理完成: 任务3\n",
      "\n",
      "异步调用完成: 任务3\n",
      "异步调用完成: 任务2\n",
      "处理完成: 任务1\n",
      "异步调用完成: 任务1\n",
      "所有结果: ['结果: 任务1', '结果: 任务2', '结果: 任务3']\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class SimpleRunnable:\n",
    "    def invoke(self, input):\n",
    "        \"\"\"同步方法 - 模拟耗时操作\"\"\"\n",
    "        import time\n",
    "        print(f\"开始处理: {input}\")\n",
    "        time.sleep(2)  # 模拟耗时操作\n",
    "        print(f\"处理完成: {input}\")\n",
    "        return f\"结果: {input}\"\n",
    "    \n",
    "    async def ainvoke(self, input):\n",
    "        \"\"\"异步方法 - 使用线程池执行同步方法\"\"\"\n",
    "        print(f\"异步调用开始: {input}\")\n",
    "        \n",
    "        # 步骤1: 获取事件循环\n",
    "        loop = asyncio.get_running_loop()\n",
    "        print(f\"获取到事件循环: {loop}\")\n",
    "        \n",
    "        # 步骤2: 在线程池中执行同步方法\n",
    "        print(f\"提交到线程池执行...\")\n",
    "        result = await loop.run_in_executor(\n",
    "            None,           # 默认线程池\n",
    "            self.invoke,    # 同步方法\n",
    "            input          # 参数\n",
    "        )\n",
    "        \n",
    "        print(f\"异步调用完成: {input}\")\n",
    "        return result\n",
    "    \n",
    "\n",
    "# 使用示例\n",
    "async def demo():\n",
    "    runnable = SimpleRunnable()\n",
    "    \n",
    "    # 并发执行多个异步调用\n",
    "    tasks = [\n",
    "        runnable.ainvoke(\"任务1\"),\n",
    "        runnable.ainvoke(\"任务2\"), \n",
    "        runnable.ainvoke(\"任务3\")\n",
    "    ]\n",
    "    \n",
    "    # 等待所有任务完成\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    print(f\"所有结果: {results}\")\n",
    "\n",
    "\n",
    "# 运行演示\n",
    "# asyncio.run(demo())\n",
    "# 运行演示 - Jupyter环境中直接使用await\n",
    "await demo()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. 为什么使用 run_in_executor？\n",
    "\n",
    "问题：同步代码在异步环境中的困境\n",
    "\n",
    "```python\n",
    "# 错误的做法 - 会阻塞整个事件循环\n",
    "async def bad_example():\n",
    "    result1 = some_sync_function()  # 阻塞2秒\n",
    "    result2 = another_sync_function()  # 又阻塞2秒\n",
    "    # 总共需要4秒，无法并发\n",
    "```\n",
    "解决方案：线程池执行\n",
    "```python\n",
    "# 正确的做法 - 使用线程池\n",
    "async def good_example():\n",
    "    loop = asyncio.get_running_loop()\n",
    "    \n",
    "    # 并发执行，总共只需要2秒\n",
    "    task1 = loop.run_in_executor(None, some_sync_function)\n",
    "    task2 = loop.run_in_executor(None, another_sync_function)\n",
    "    \n",
    "    result1, result2 = await asyncio.gather(task1, task2)\n",
    "```\n",
    "#### 5. 事件循环的工作原理\n",
    "```python\n",
    "# 事件循环的简化模型\n",
    "class EventLoop:\n",
    "    def __init__(self):\n",
    "        self.tasks = []\n",
    "        self.thread_pool = ThreadPoolExecutor()\n",
    "    \n",
    "    def run_in_executor(self, executor, func, *args):\n",
    "        \"\"\"在线程池中执行同步函数\"\"\"\n",
    "        # 1. 提交到线程池\n",
    "        future = self.thread_pool.submit(func, *args)\n",
    "        \n",
    "        # 2. 创建异步 Future\n",
    "        async_future = asyncio.Future()\n",
    "        \n",
    "        # 3. 当线程池任务完成时，设置异步 Future 的结果\n",
    "        def on_done(thread_future):\n",
    "            try:\n",
    "                result = thread_future.result()\n",
    "                async_future.set_result(result)\n",
    "            except Exception as e:\n",
    "                async_future.set_exception(e)\n",
    "        \n",
    "        future.add_done_callback(on_done)\n",
    "        return async_future\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4de0f62b4c52d50"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 6. 何时使用异步\n",
    "\n",
    "```python\n",
    "# LLM 调用通常是网络请求，天然适合异步\n",
    "class AsyncLLM:\n",
    "    async def ainvoke(self, prompt):\n",
    "        # 直接使用异步 HTTP 客户端\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            response = await session.post(api_url, json={\"prompt\": prompt})\n",
    "            return await response.json()\n",
    "\n",
    "# 但某些组件可能只有同步实现\n",
    "class SyncProcessor:\n",
    "    def invoke(self, input):\n",
    "        # 复杂的同步处理逻辑\n",
    "        return process_data(input)\n",
    "    \n",
    "    async def ainvoke(self, input):\n",
    "        # 使用线程池包装同步方法\n",
    "        loop = asyncio.get_running_loop()\n",
    "        return await loop.run_in_executor(None, self.invoke, input)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a600835c80ead1f3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## stream方法\n",
    "1. stream 方法返回迭代器：使用 yield 逐步返回结果\n",
    "2. 实时显示：用 print(chunk, end=\"\", flush=True) 实现逐字显示效果\n",
    "3. 与 invoke 对比：invoke 一次性返回，stream 分步返回\n",
    "\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Stream 方法\n",
    "\"\"\"\n",
    "from langchain_core.runnables import Runnable\n",
    "from typing import Iterator\n",
    "import time\n",
    "\n",
    "class SimpleStreamRunnable(Runnable[str, str]):\n",
    "    \"\"\"简单的流式输出演示\"\"\"\n",
    "    \n",
    "    def invoke(self, input: str) -> str:\n",
    "        \"\"\"普通调用 - 一次性返回完整结果\"\"\"\n",
    "        return f\"完整处理结果: {input}\"\n",
    "    \n",
    "    def stream(self, input: str) -> Iterator[str]:\n",
    "        \"\"\"流式调用 - 逐步返回部分结果\"\"\"\n",
    "        words = f\"逐步处理结果: {input}\".split()\n",
    "        \n",
    "        for word in words:\n",
    "            time.sleep(0.5)  # 模拟处理延迟\n",
    "            yield word + \" \"  # 逐个返回单词\n",
    "\n",
    "# 使用演示\n",
    "def demo_stream():\n",
    "    runnable = SimpleStreamRunnable()\n",
    "    \n",
    "    print(\"=== 普通调用 ===\")\n",
    "    result = runnable.invoke(\"测试文本\")\n",
    "    print(result)\n",
    "    \n",
    "    print(\"\\n=== 流式调用 ===\")\n",
    "    for chunk in runnable.stream(\"测试文本\"):\n",
    "        print(chunk, end=\"\", flush=True)  # 实时显示，不换行\n",
    "    \n",
    "    print(\"\\n\\n演示完成\")\n",
    "\n",
    "# 运行演示\n",
    "demo_stream()\n",
    "```\n",
    "\n",
    "**astream 是 stream 的异步版本，astream的默认实现调用了ainvoke**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e2b583e1809fe3e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### batch方法\n",
    "\n",
    "batch方法用于处理多个输入（批处理）\n",
    "\n",
    "工程价值：\n",
    "- 性能优化：批量处理比逐个处理快3-5倍\n",
    "- 资源管理：分批处理避免内存溢出\n",
    "- 错误处理：批次失败时自动降级为单个处理\n",
    "- 监控报告：提供详细的处理统计和分析\n",
    "\n",
    "适用场景：\n",
    "- 大量文档批量分析\n",
    "- 客户反馈批量处理\n",
    "- 数据清洗和标注\n",
    "- 内容审核和分类\n",
    "- 报告自动生成"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "facc18e4201ac373"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8b3b132446e704a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-15T07:21:01.340205Z",
     "start_time": "2025-12-15T07:21:01.322436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 使用自定义 Runnable ===\n",
      "输入: ['报告A', '合同B', '邮件C']\n",
      "批量处理结果:\n",
      "  分析结果: 报告A -> 类型:文档, 重要性:中等\n",
      "  分析结果: 合同B -> 类型:文档, 重要性:中等\n",
      "  分析结果: 邮件C -> 类型:文档, 重要性:中等\n",
      "\n",
      "=== 使用 PromptTemplate ===\n",
      "输入: ['项目进展', '用户反馈', '市场分析']\n",
      "提示词批量生成结果:\n",
      "  text='请分析: 项目进展'\n",
      "  text='请分析: 用户反馈'\n",
      "  text='请分析: 市场分析'\n",
      "\n",
      "=== 链式 batch ===\n",
      "链式批量处理结果:\n",
      "  分析结果: text='请分析: 项目进展' -> 类型:文档, 重要性:中等\n",
      "  分析结果: text='请分析: 用户反馈' -> 类型:文档, 重要性:中等\n",
      "  分析结果: text='请分析: 市场分析' -> 类型:文档, 重要性:中等\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LangChain 原生 batch 方法演示\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import Tongyi\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "class SimpleAnalyzer(Runnable[str, str]):\n",
    "    \"\"\"简单分析器\"\"\"\n",
    "    \n",
    "    def invoke(self, input: str, config: Optional[Dict[str, Any]] = None) -> str:\n",
    "        \"\"\"单个分析\"\"\"\n",
    "        return f\"分析结果: {input} -> 类型:文档, 重要性:中等\"\n",
    "    \n",
    "def demo_langchain_batch():\n",
    "    \"\"\"LangChain batch 方法演示\"\"\"\n",
    "    \n",
    "    # 1. 使用自定义 Runnable\n",
    "    analyzer = SimpleAnalyzer()\n",
    "    inputs = [\"报告A\", \"合同B\", \"邮件C\"]\n",
    "    \n",
    "    print(\"=== 使用自定义 Runnable ===\")\n",
    "    print(f\"输入: {inputs}\")\n",
    "    \n",
    "    # LangChain 的 batch 方法\n",
    "    results = analyzer.batch(inputs)\n",
    "    \n",
    "    print(\"批量处理结果:\")\n",
    "    for result in results:\n",
    "        print(f\"  {result}\")\n",
    "    \n",
    "    # 2. 使用 PromptTemplate\n",
    "    print(\"\\n=== 使用 PromptTemplate ===\")\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=\"请分析: {text}\"\n",
    "    )\n",
    "    \n",
    "    texts = [\"项目进展\", \"用户反馈\", \"市场分析\"]\n",
    "    inputs_dict = [{\"text\": text} for text in texts]\n",
    "    \n",
    "    print(f\"输入: {texts}\")\n",
    "    \n",
    "    # PromptTemplate 的 batch 方法\n",
    "    prompt_results = prompt.batch(inputs_dict)\n",
    "    \n",
    "    print(\"提示词批量生成结果:\")\n",
    "    for result in prompt_results:\n",
    "        print(f\"  {result}\")\n",
    "    \n",
    "    # 3. 链式 batch\n",
    "    print(\"\\n=== 链式 batch ===\")\n",
    "    \n",
    "    # 创建简单链\n",
    "    chain = prompt | analyzer\n",
    "    \n",
    "    # 链的 batch 方法\n",
    "    chain_results = chain.batch(inputs_dict)\n",
    "    \n",
    "    print(\"链式批量处理结果:\")\n",
    "    for result in chain_results:\n",
    "        print(f\"  {result}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_langchain_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 并行组合\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# 并行执行多个分支\n",
    "parallel = RunnableParallel({\n",
    "    \"summary\": summary_chain,\n",
    "    \"keywords\": keyword_chain,\n",
    "    \"sentiment\": sentiment_chain\n",
    "})\n",
    "\n",
    "# 输入会同时发送给所有分支\n",
    "result = parallel.invoke({\"text\": \"分析这段文本\"})\n",
    "# 输出: {\"summary\": \"...\", \"keywords\": [...], \"sentiment\": \"positive\"}\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d22395a42631be85"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 条件分支\n",
    "\n",
    "RunnableBranch 是一种重要的路由机制，它根据给定的条件选择不同的处理路径。在运行过程中还能根据输入动态选择不同的执行路径。\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "# 根据条件选择不同处理路径\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: x[\"type\"] == \"question\", qa_chain),\n",
    "    (lambda x: x[\"type\"] == \"summary\", summary_chain),\n",
    "    default_chain\n",
    ")\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7e8a722a623d9a3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RunnableBranch 条件分支演示 ===\n",
      "\n",
      "--- 测试用例 1 ---\n",
      "类型: question\n",
      "内容: 什么是人工智能？\n",
      "处理结果: 人工智能（Artificial Intelligence，简称 AI）是指由人制造出来的机器所表现出来的智能。它使机器能够模拟、延伸或扩展人类的感知、学习、推理、规划、理解语言、识别图像、解决问题和决策等能力。\n",
      "\n",
      "人工智能的核心目标是让机器具备类似人类的认知功能。根据其能力和应用范围，人工智能通常分为以下几类：\n",
      "\n",
      "1. **弱人工智能（Narrow AI）**：专注于完成特定任务的人工智能，例如语音助手（如Siri）、图像识别系统、推荐算法等。这类AI在特定领域表现出色，但不具备通用智能。\n",
      "\n",
      "2. **强人工智能（General AI）**：指具备与人类相当或超越人类的全面认知能力，能够在各种复杂环境中自主学习和适应。目前仍处于理论和研究阶段。\n",
      "\n",
      "3. **超级人工智能（Superintelligent AI）**：在所有方面都远超人类智能水平的AI，属于未来可能的发展方向，尚无现实实现。\n",
      "\n",
      "人工智能的技术基础包括机器学习（尤其是深度学习）、自然语言处理、计算机视觉、知识表示与推理、机器人技术等。\n",
      "\n",
      "总之，人工智能是一门跨学科的科学技术，致力于让机器“像人一样思考”或“像人一样行动”，正在广泛应用于医疗、交通、金融、教育、娱乐等各个领域，深刻改变着人类社会的生活方式。\n",
      "\n",
      "--- 测试用例 2 ---\n",
      "类型: summary\n",
      "内容: 人工智能是计算机科学的一个分支，致力于创建能够执行通常需要人类智能的任务的系统。它包括机器学习、深度学习、自然语言处理等多个子领域。\n",
      "处理结果: 人工智能是计算机科学的分支，旨在开发能执行需人类智能任务的系统，涵盖机器学习、深度学习、自然语言处理等多个子领域。\n",
      "\n",
      "--- 测试用例 3 ---\n",
      "类型: other\n",
      "内容: 今天天气很好\n",
      "处理结果: “今天天气很好”这句话是一个简单的陈述句，表达了说话者对当前天气状况的主观评价。我们可以从多个角度进行分析：\n",
      "\n",
      "1. **语义分析**：\n",
      "   - “今天”：表示时间，指说话当天。\n",
      "   - “天气”：指大气状况，包括温度、湿度、降水、风力、云量等自然现象。\n",
      "   - “很好”：是评价性词语，表示积极、正面的感受。具体好在哪里（如晴朗、温暖、凉爽等）未明确说明，具有一定的主观性和模糊性。\n",
      "\n",
      "2. **语用分析**：\n",
      "   - 这句话常用于日常寒暄，可能并不只是传达气象信息，更是一种社交开场白，用于拉近人际关系。\n",
      "   - 在不同语境中可能隐含不同意图：例如，暗示适合外出、表达心情愉悦，或为后续话题做铺垫（如“今天天气很好，要不要一起去散步？”）。\n",
      "\n",
      "3. **情感与情绪表达**：\n",
      "   - 体现说话人当下的积极情绪。良好的天气常与愉悦心情相关联。\n",
      "   - 可能反映一种对自然环境的欣赏或满足感。\n",
      "\n",
      "4. **文化与语境因素**：\n",
      "   - 在气候多变或长期阴雨的地区，说“天气很好”可能带有更强的欣喜意味。\n",
      "   - 在某些文化中，谈论天气是避免直接切入敏感话题的礼貌方式。\n",
      "\n",
      "5. **语言简洁性**：\n",
      "   - 句子结构简单，主谓宾完整，易于理解，符合汉语口语表达习惯。\n",
      "   - 缺少具体细节（如气温多少、是否阳光明媚），依赖听者结合上下文或实际环境理解。\n",
      "\n",
      "总结：\n",
      "“今天天气很好”虽是一句简短的日常表达，但承载了信息传递、情感表达和社交互动多重功能。其意义不仅在于描述天气本身，更在于它在人际交流中的桥梁作用。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaorj/PycharmProjects/Learning/ai-quickstart/.venv/lib/python3.12/site-packages/langchain_core/callbacks/base.py:922: RuntimeWarning: coroutine 'demo' was never awaited\n",
      "  return self.__class__(\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批量结果 1: 机器学习（Machine Learning）是人工智能的一个分支，它致力于让计算机系统能够从数据中自动学习并改进性能，而无需被明确编程去执行特定任务。\n",
      "\n",
      "简单来说，机器学习通过设计算法，使计算机能够：\n",
      "\n",
      "1. **从数据中识别模式**：利用大量历史数据，发现其中的规律和关联。\n",
      "2. **做出预测或决策**：基于学到的模式，对新的、未见过的数据进行预测或分类。\n",
      "3. **不断优化自身表现**：随着更多数据的输入和反馈，模型可以持续改进其准确性。\n",
      "\n",
      "常见的机器学习类型包括：\n",
      "\n",
      "- **监督学习**：使用带有标签的数据训练模型（如已知图片中的动物是猫还是狗），用于分类或回归任务。\n",
      "- **无监督学习**：处理没有标签的数据，发现数据中的结构或聚类（如客户分群）。\n",
      "- **强化学习**：通过试错和奖励机制训练模型在环境中做出最优决策（如游戏AI或机器人控制）。\n",
      "\n",
      "机器学习广泛应用于图像识别、语音助手、推荐系统、医疗诊断、金融风控等领域，是现代智能技术的核心之一。\n",
      "批量结果 2: 机器学习是人工智能的一个重要分支，致力于通过数据和算法使计算机系统具备自动学习和改进的能力，而无需显式编程。它通过分析大量数据，识别其中的模式并做出预测或决策，广泛应用于图像识别、自然语言处理、推荐系统等领域。主要学习方式包括监督学习、无监督学习和强化学习。随着数据量的增长和计算能力的提升，机器学习在近年来取得了显著进展，成为推动人工智能发展的重要力量。\n",
      "批量结果 3: 您提到“请分析以下内容：随机内容”，但尚未提供具体需要分析的文本或信息。为了更好地帮助您，请补充以下任一类型的详细内容：\n",
      "\n",
      "1. **待分析的文本/数据**（例如：一段文章、对话记录、用户反馈等）\n",
      "2. **分析目标**（例如：情感倾向、主题归纳、逻辑漏洞、语言风格、数据规律等）\n",
      "3. **特殊需求**（如需用特定理论框架、技术工具或呈现形式）\n",
      "\n",
      "示例：\n",
      "- 若您想分析一段社交媒体评论的情感，可提供具体文字；\n",
      "- 若需识别数据中的异常模式，请上传表格或描述数据特征。\n",
      "\n",
      "补充信息后，我将为您生成针对性的深度分析。\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RunnableBranch 条件分支演示\n",
    "\"\"\"\n",
    "from langchain_core.runnables import RunnableBranch, RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import Tongyi\n",
    "from typing import Dict, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# === 前面的准备工作 ===\n",
    "\n",
    "# 1. 创建不同类型的处理链\n",
    "# 问答链\n",
    "qa_prompt = PromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    template=\"请回答以下问题：{content}\"\n",
    ")\n",
    "\n",
    "qa_llm = Tongyi(model_name=\"qwen-max\", temperature=0.1)\n",
    "qa_chain = qa_prompt | qa_llm\n",
    "\n",
    "# 摘要链\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"content\"], \n",
    "    template=\"请总结以下内容：{content}\"\n",
    ")\n",
    "summary_llm = Tongyi(model_name=\"qwen-max\", temperature=0.3)\n",
    "summary_chain = summary_prompt | summary_llm\n",
    "\n",
    "# 默认处理链\n",
    "default_prompt = PromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    template=\"请分析以下内容：{content}\"\n",
    ")\n",
    "default_llm = Tongyi(model_name=\"qwen-max\", temperature=0.5)\n",
    "default_chain = default_prompt | default_llm\n",
    "\n",
    "# === 核心代码：创建条件分支 ===\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: x[\"type\"] == \"question\", qa_chain),\n",
    "    (lambda x: x[\"type\"] == \"summary\", summary_chain),\n",
    "    default_chain  # 默认分支\n",
    ")\n",
    "\n",
    "# === 后面的使用方法 ===\n",
    "\n",
    "# 使用示例\n",
    "def demo_branch():\n",
    "    \"\"\"演示条件分支的使用\"\"\"\n",
    "    \n",
    "    # 测试数据\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"type\": \"question\",\n",
    "            \"content\": \"什么是人工智能？\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"summary\", \n",
    "            \"content\": \"人工智能是计算机科学的一个分支，致力于创建能够执行通常需要人类智能的任务的系统。它包括机器学习、深度学习、自然语言处理等多个子领域。\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"other\",\n",
    "            \"content\": \"今天天气很好\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"=== RunnableBranch 条件分支演示 ===\")\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\n--- 测试用例 {i} ---\")\n",
    "        print(f\"类型: {test_case['type']}\")\n",
    "        print(f\"内容: {test_case['content']}\")\n",
    "        \n",
    "        # 执行条件分支\n",
    "        result = branch.invoke(test_case)\n",
    "        print(f\"处理结果: {result}\")\n",
    "        \n",
    "# 更复杂的条件分支示例\n",
    "def create_advanced_branch():\n",
    "    \"\"\"创建更复杂的条件分支\"\"\"\n",
    "    # 定义更多条件\n",
    "    advanced_branch = RunnableBranch(\n",
    "        # 条件1：问题类型\n",
    "        (lambda x: x[\"type\"] == \"question\", qa_chain),\n",
    "        \n",
    "        # 条件2：摘要类型\n",
    "        (lambda x: x[\"type\"] == \"summary\", summary_chain),\n",
    "        \n",
    "        # 条件3：长文本（超过100字）\n",
    "        (lambda x: len(x[\"content\"]) > 100, \n",
    "         PromptTemplate(input_variables=[\"content\"], template=\"这是长文本，请详细分析：{content}\") | default_llm),\n",
    "        \n",
    "        # 条件4：包含特定关键词\n",
    "        (lambda x: \"紧急\" in x[\"content\"], \n",
    "         PromptTemplate(input_variables=[\"content\"], template=\"紧急处理：{content}\") | default_llm),\n",
    "        \n",
    "        # 默认分支\n",
    "        default_chain\n",
    "    )\n",
    "    \n",
    "    return advanced_branch\n",
    "\n",
    "\n",
    "# 异步使用\n",
    "async def demo_async_branch():\n",
    "    \"\"\"异步使用条件分支\"\"\"\n",
    "    test_input = {\n",
    "        \"type\": \"question\",\n",
    "        \"content\": \"如何使用 LangChain？\"\n",
    "    }\n",
    "    \n",
    "    # 异步调用\n",
    "    result = await branch.ainvoke(test_input)\n",
    "    print(f\"异步结果: {result}\")\n",
    "\n",
    "# 批量处理\n",
    "def demo_batch_branch():\n",
    "    \"\"\"批量处理演示\"\"\"\n",
    "    batch_inputs = [\n",
    "        {\"type\": \"question\", \"content\": \"什么是机器学习？\"},\n",
    "        {\"type\": \"summary\", \"content\": \"机器学习是人工智能的一个重要分支...\"},\n",
    "        {\"type\": \"other\", \"content\": \"随机内容\"}\n",
    "    ]\n",
    "    \n",
    "    # 批量处理\n",
    "    results = branch.batch(batch_inputs)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"批量结果 {i+1}: {result}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 运行演示\n",
    "    demo_branch()\n",
    "    \n",
    "    # 高级分支演示\n",
    "    advanced_branch = create_advanced_branch()\n",
    "    \n",
    "    # 批量处理演示\n",
    "    demo_batch_branch()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-12-15T07:54:28.384567Z",
     "start_time": "2025-12-15T07:54:04.208851Z"
    }
   },
   "id": "c90979d6dc59097a",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 输入输出处理\n",
    "\n",
    "RunnableLambda（自定义函数）\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# 将普通函数转换为 Runnable，实现与 LCEL 组件兼容\n",
    "def preprocess(text):\n",
    "    return text.upper().strip()\n",
    "\n",
    "preprocessor = RunnableLambda(preprocess)\n",
    "chain = preprocessor | model\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cfaae82e7971db1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 错误处理和容错\n",
    "重试机制\n",
    "```python\n",
    "from langchain_core.runnables import RunnableRetry\n",
    "\n",
    "# 自动重试\n",
    "retry_runnable = RunnableRetry(\n",
    "    bound=model,\n",
    "    max_attempts=3,\n",
    "    wait_exponential_jitter=True\n",
    ")\n",
    "```\n",
    "\n",
    "回退机制\n",
    "```python\n",
    "# 主要方法失败时使用备用方法\n",
    "fallback_chain = primary_model.with_fallbacks([backup_model, simple_template])\n",
    "```\n",
    "\n",
    "超时控制\n",
    "```python\n",
    "# 设置执行超时\n",
    "with_timeout = model.with_timeout(30.0)  # 30秒超时\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea186b0b72fa396a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\"\"\"\n",
    "企业级 LangChain 应用 - 智能客服系统\n",
    "兼容当前版本，包含完整的错误处理和容错机制\n",
    "\"\"\"\n",
    "- p14-kefu"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbaae38d1fdf5f11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. 配置和定制\n",
    "运行时配置\n",
    "\n",
    "```python\n",
    "# 传递配置参数\n",
    "config = {\"temperature\": 0.7, \"max_tokens\": 100}\n",
    "result = model.invoke(input, config=config)\n",
    "\n",
    "# 绑定默认配置\n",
    "bound_model = model.bind(temperature=0.7)\n",
    "```\n",
    "\n",
    "\n",
    "标签和元数据\n",
    "\n",
    "```python\n",
    "# 添加标签用于追踪\n",
    "tagged_chain = chain.with_tags([\"production\", \"v1.0\"])\n",
    "\n",
    "# 添加元数据\n",
    "chain_with_metadata = chain.with_metadata({\"version\": \"1.0\", \"author\": \"team\"})\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfe379da4004c01e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. 调试和监控\n",
    "中间结果获取\n",
    "\n",
    "```python\n",
    "# 获取每一步的输出\n",
    "def debug_step(step_output):\n",
    "    print(f\"步骤输出: {step_output}\")\n",
    "    return step_output\n",
    "\n",
    "debug_chain = prompt | RunnableLambda(debug_step) | model\n",
    "```\n",
    "\n",
    "性能监控\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "def timing_wrapper(runnable):\n",
    "    def timed_invoke(input):\n",
    "        start = time.time()\n",
    "        result = runnable.invoke(input)\n",
    "        print(f\"执行时间: {time.time() - start:.2f}秒\")\n",
    "        return result\n",
    "    return RunnableLambda(timed_invoke)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d400194411bb311"
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. 自定义 Runnable\n",
    "基本实现\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "class CustomProcessor(Runnable[str, Dict]):\n",
    "    def invoke(self, input: str, config=None) -> Dict:\n",
    "        # 自定义处理逻辑\n",
    "        return {\"processed\": input.upper(), \"length\": len(input)}\n",
    "    \n",
    "    async def ainvoke(self, input: str, config=None) -> Dict:\n",
    "        # 异步版本\n",
    "        return self.invoke(input, config)\n",
    "```\n",
    "\n",
    "\n",
    "完整实现模板\n",
    "```python\n",
    "class AdvancedRunnable(Runnable[InputType, OutputType]):\n",
    "    def __init__(self, param1, param2):\n",
    "        self.param1 = param1\n",
    "        self.param2 = param2\n",
    "    \n",
    "    def invoke(self, input: InputType, config=None) -> OutputType:\n",
    "        # 同步执行逻辑\n",
    "        pass\n",
    "    \n",
    "    async def ainvoke(self, input: InputType, config=None) -> OutputType:\n",
    "        # 异步执行逻辑\n",
    "        pass\n",
    "    \n",
    "    def batch(self, inputs: List[InputType], config=None) -> List[OutputType]:\n",
    "        # 批量处理逻辑\n",
    "        return [self.invoke(inp, config) for inp in inputs]\n",
    "    \n",
    "    def stream(self, input: InputType, config=None):\n",
    "        # 流式处理逻辑\n",
    "        yield from self._stream_generator(input)\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da18c4e26d6c7bcc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 最佳实践\n",
    "\n",
    "```python\n",
    "# 1. 明确类型定义\n",
    "class MyChain(Runnable[Dict[str, str], Dict[str, Any]]):\n",
    "    pass\n",
    "\n",
    "# 2. 合理使用并行\n",
    "parallel_processing = RunnableParallel({\n",
    "    \"fast_analysis\": quick_model,\n",
    "    \"detailed_analysis\": detailed_model\n",
    "})\n",
    "\n",
    "# 3. 添加错误处理\n",
    "robust_chain = (\n",
    "    preprocessing |\n",
    "    model.with_fallbacks([backup_model]) |\n",
    "    postprocessing\n",
    ")\n",
    "\n",
    "# 4. 使用配置管理\n",
    "configurable_chain = model.configurable_fields(\n",
    "    temperature=ConfigurableField(id=\"temp\", name=\"Temperature\")\n",
    ")\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "703a1a496aec73fe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e1f0df44dffafb52"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
